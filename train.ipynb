{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.autograd import grad as torch_grad\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, init_type='xavier', gain=0.02):\n",
    "   '''\n",
    "   initialize network's weights\n",
    "   init_type: normal | xavier | kaiming | orthogonal\n",
    "   '''\n",
    "\n",
    "   def init_func(m):\n",
    "       classname = m.__class__.__name__\n",
    "       if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "           if init_type == 'normal':\n",
    "               nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "           elif init_type == 'xavier':\n",
    "#                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "           elif init_type == 'kaiming':\n",
    "               nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "           elif init_type == 'orthogonal':\n",
    "               nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "           if hasattr(m, 'bias') and m.bias is not None:\n",
    "               nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "       elif classname.find('BatchNorm2d') != -1:\n",
    "           nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "           nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "   model.apply(init_func)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "            @notice: avoid in-place ops.\n",
    "            https://discuss.pytorch.org/t/encounter-the-runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation/836/3\n",
    "        \"\"\"\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        tmp  = torch.mul(x, x) # or x ** 2\n",
    "        tmp1 = torch.rsqrt(torch.mean(tmp, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "        return x * tmp1\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder, self).__init__()\n",
    "        self.conv_dim = 64\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 16, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(16, 256, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(128, 64, 3, 1, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 3, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        out = self.decoder(feat) \n",
    "        return feat, out\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "class upblock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(upblock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            spectral_norm(nn.Conv2d(in_channels, out_channels, 3, 1, 1)),\n",
    "#             PixelNorm(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),\n",
    "            spectral_norm(nn.Conv2d(out_channels, out_channels, 3, 1, 1)),\n",
    "#             PixelNorm(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self, num_z, imsize=64):\n",
    "        super(generator, self).__init__()\n",
    "        conv_dim = 64\n",
    "        \n",
    "        num_up = int(math.log(imsize, 2)) - 4 #start from 4x4\n",
    "        in_dim = conv_dim*2**(num_up-1)\n",
    "        out_dim = in_dim\n",
    "        \n",
    "        self.proj = spectral_norm(nn.Linear(num_z, in_dim * 4 * 4))\n",
    "        decoder = [nn.BatchNorm2d(in_dim), nn.ReLU(True)]\n",
    "        for i in range(num_up):\n",
    "            decoder.append(upblock(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "            out_dim = out_dim // 2\n",
    "            \n",
    "        decoder += [spectral_norm(nn.Conv2d(in_dim, 16, 3, 1, 1)), nn.Tanh()]\n",
    "        self.decoder = nn.Sequential(*decoder)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.proj(x).view(x.size(0),-1,4,4)\n",
    "        out = self.decoder(out) \n",
    "        return out\n",
    "\n",
    "class code_discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, imsize=64):\n",
    "        super(code_discriminator, self).__init__()\n",
    "        num_down = int(math.log(imsize, 2)) - 4 #start from 4x4\n",
    "        in_dim = 16\n",
    "        out_dim = 64\n",
    "        \n",
    "        encoder = []\n",
    "        \n",
    "        for i in range(num_down):\n",
    "            encoder += [spectral_norm(nn.Conv2d(in_dim, out_dim, 3, 1, 1)),\n",
    "                            nn.BatchNorm2d(out_dim),\n",
    "                            nn.LeakyReLU(0.2, True),\n",
    "                            spectral_norm(nn.Conv2d(out_dim, out_dim*2, 3, 2, 1)),\n",
    "                            nn.BatchNorm2d(out_dim*2),\n",
    "                            nn.LeakyReLU(0.2, True),\n",
    "                           ]\n",
    "            in_dim = out_dim*2\n",
    "            out_dim *= 2\n",
    "            \n",
    "        encoder += [spectral_norm(nn.Conv2d(in_dim, out_dim, 3, 1, 1)),\n",
    "                        nn.BatchNorm2d(out_dim),\n",
    "                        nn.LeakyReLU(0.2, True),\n",
    "                        Flatten(), \n",
    "                        spectral_norm(nn.Linear(out_dim*4*4, 1)),\n",
    "                       ]\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder)\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        return out\n",
    "    \n",
    "class im_discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=64):\n",
    "        super(im_discriminator, self).__init__()\n",
    "        conv_dim = d        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, conv_dim, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(conv_dim, conv_dim * 2, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(conv_dim * 2, conv_dim* 4, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(conv_dim * 4, conv_dim * 8, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(conv_dim * 8, conv_dim * 8, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            Flatten(), \n",
    "            nn.Linear(conv_dim*8*4*4, 1),\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class im_dataset(Dataset):\n",
    "    def __init__(self, real_dir, imsize):\n",
    "        self.real_dir = real_dir\n",
    "        self.imgpaths = self.get_imgpaths()\n",
    "\n",
    "        self.preprocessing = transforms.Compose([\n",
    "                transforms.Resize(imsize),\n",
    "                transforms.CenterCrop(imsize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "    def get_imgpaths(self):\n",
    "        real_paths = sorted(glob.glob('%s/*.jpg'%self.real_dir, recursive=True))\n",
    "        return real_paths\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        truepath = self.imgpaths[idx]\n",
    "        true_im = self.preprocessing(Image.open(truepath))\n",
    "        if true_im.size(0) == 1:\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        return true_im\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgpaths)\n",
    "\n",
    "class code_dataset(Dataset):\n",
    "    def __init__(self, real_dir):\n",
    "        self.real_dir = real_dir\n",
    "        self.imgpaths = self.get_imgpaths()\n",
    "\n",
    "    def get_imgpaths(self):\n",
    "        real_paths = sorted(glob.glob('%s/*.npz'%self.real_dir))\n",
    "        return real_paths\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.imgpaths[idx]\n",
    "        code = np.load(path)\n",
    "        \n",
    "        return torch.FloatTensor(code)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgpaths)\n",
    "\n",
    "def generate_recon(epoch, img):\n",
    "    encode_model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat, out = encode_model(img)\n",
    "        pic = to_img(out.cpu().data)\n",
    "        save_image(pic, '%s/recon_%d.png'%(out_dir, epoch))\n",
    "        pic = to_img(img.cpu().data)\n",
    "        save_image(pic, '%s/original_%d.png'%(out_dir, epoch))\n",
    "    encode_model.train()\n",
    "        \n",
    "def generate_gan(epoch, img, fixed_z):\n",
    "#     code_gen.eval()\n",
    "    encode_model.eval()\n",
    "    with torch.no_grad():\n",
    "        code = code_gen(fixed_z)\n",
    "        out = encode_model.decode(code)\n",
    "        pic = to_img(out.cpu().data)[:30]\n",
    "        save_image(pic, '%s/local_%d.png'%(out_dir, epoch))\n",
    "        \n",
    "        _, out = encode_model(img)\n",
    "        pic = to_img(out.cpu().data)[:30]\n",
    "        save_image(pic, '%s/large_recon_%d.png'%(out_dir, epoch))\n",
    "\n",
    "    code_gen.train()\n",
    "\n",
    "def adjust_learning_rate(optimizers, epoch):        \n",
    "    for optimizer in optimizers:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "            if epoch == 10:\n",
    "                new_lr = current_lr / 10\n",
    "            else:\n",
    "                new_lr = current_lr\n",
    "            param_group['lr'] = new_lr\n",
    "            \n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "    Adversarial loss\n",
    "    https://arxiv.org/abs/1711.10337\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, type='nsgan', target_real_label=1.0, target_fake_label=0.0):\n",
    "        r\"\"\"\n",
    "        type = nsgan | lsgan | hinge\n",
    "        \"\"\"\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "\n",
    "        self.type = type\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "\n",
    "        if type == 'nsgan':\n",
    "            self.criterion = nn.BCELoss()\n",
    "\n",
    "        elif type == 'lsgan':\n",
    "            self.criterion = nn.MSELoss()\n",
    "\n",
    "        elif type == 'hinge':\n",
    "            self.criterion = nn.ReLU()\n",
    "\n",
    "    def __call__(self, outputs, is_real, is_disc=None):\n",
    "        if self.type == 'hinge':\n",
    "            if is_disc:\n",
    "                if is_real:\n",
    "                    outputs = -outputs\n",
    "                return self.criterion(1 + outputs).mean()\n",
    "            else:\n",
    "                return (-outputs).mean()\n",
    "        elif self.type == 'wgan-gp':\n",
    "            if is_real:\n",
    "                outputs = -outputs\n",
    "            return outputs.mean()\n",
    "        else:\n",
    "            labels = (self.real_label if is_real else self.fake_label).expand_as(outputs)\n",
    "            if self.type == 'nsgan':\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            return loss\n",
    "\n",
    "        \n",
    "def gradient_penalty(real_data, generated_data, disc):\n",
    "    batch_size = real_data.size(0)\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).cuda()\n",
    "    alpha = alpha.expand_as(real_data)\n",
    "    interpolated = (alpha * real_data + (1 - alpha) * generated_data).detach().requires_grad_()\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    prob_interpolated = disc(interpolated)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                           grad_outputs=torch.ones(prob_interpolated.size()).cuda(),\n",
    "                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "    # so flatten to easily take norm per example in batch\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "    # Return gradient penalty\n",
    "    return 10 * ((gradients_norm - 1) ** 2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './adv_training2'\n",
    "code_dir = os.path.join(out_dir, 'codes')\n",
    "os.makedirs(out_dir, exist_ok=True)    \n",
    "os.makedirs(code_dir, exist_ok=True)    \n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "dataset = im_dataset('./data/img_align_celeba', 64)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 100\n",
    "k = 256\n",
    "output_size = 256\n",
    "\n",
    "encode_model = encoder().cuda().train()\n",
    "code_gen = generator(k, output_size).cuda().train()\n",
    "\n",
    "im_disc = im_discriminator().cuda().train()\n",
    "code_disc = code_discriminator(output_size).cuda().train()\n",
    "\n",
    "encode_model.load_state_dict(torch.load('./adv_training2/encoder.pth'))\n",
    "# code_gen.load_state_dict(torch.load('./adv_training2/code_gen.pth'))\n",
    "# code_disc.load_state_dict(torch.load('./adv_training2/code_disc.pth'))\n",
    "\n",
    "loss = 'hinge'\n",
    "adv_criterion = AdversarialLoss(type=loss).cuda()\n",
    "l1_criterion = nn.L1Loss()\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encode_model.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "im_disc_optimizer = torch.optim.Adam(im_disc.parameters(), lr=4e-4, betas=(0.5, 0.999))\n",
    "\n",
    "gen_optimizer = torch.optim.Adam(code_gen.parameters(), lr=1e-4, betas=(.5, 0.999))\n",
    "code_disc_optimizer = torch.optim.Adam(code_disc.parameters(), lr=4e-4, betas=(.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train encoder - decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(15):\n",
    "#     for batch_idx, img in enumerate(dataloader):\n",
    "#         adjust_learning_rate([encoder_optimizer], epoch)      \n",
    "#         img = img.cuda()\n",
    "#         feat, out = encode_model(img)        \n",
    "#         real_out = im_disc(img)\n",
    "#         fake_out = im_disc(out.detach())\n",
    "\n",
    "#         err_real = adv_criterion(real_out, True, True)\n",
    "#         err_fake = adv_criterion(fake_out, False, True)\n",
    "#         gan_loss = err_real + err_fake\n",
    "\n",
    "#         if loss == 'wgan-gp':\n",
    "#             err_wgan = gradient_penalty(img, out.detach(), im_disc)\n",
    "#             gan_loss += err_wgan\n",
    "        \n",
    "                \n",
    "#         im_disc_optimizer.zero_grad()\n",
    "#         gan_loss.backward()\n",
    "#         im_disc_optimizer.step()\n",
    "        \n",
    "#         # generator\n",
    "#         l1 = l1_criterion(out, img)\n",
    "#         fake_out = im_disc(out)\n",
    "#         adv_loss = adv_criterion(fake_out, True, False)\n",
    "#         g_loss = l1 + 0.01*adv_loss\n",
    "        \n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         g_loss.backward()\n",
    "#         encoder_optimizer.step()\n",
    "\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print(epoch, gan_loss.item(), l1.item(), adv_loss.item())\n",
    "#             generate_recon(epoch, img)\n",
    "            \n",
    "#     if epoch % 1 == 0:\n",
    "#         torch.save(encode_model.state_dict(), '%s/encode_model.pth'%out_dir)\n",
    "#         torch.save(im_disc.state_dict(), '%s/im_disc.pth'%out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert high resolution images to code and save it into a folder for quicker training. How to compress np arrays efficiently? Takes too much disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = im_dataset('./data/img_align_celeba', output_size)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# count = 0\n",
    "# for batch_idx, img in enumerate(dataloader):\n",
    "#     img = img.cuda()\n",
    "#     with torch.no_grad():\n",
    "#         real_feat, _ = encode_model(img) \n",
    "#         real_feat = real_feat.cpu().numpy()\n",
    "#         for i in range(real_feat.shape[0]):\n",
    "#             np.savez_compressed(f'{code_dir}/{count}.npz', real_feat[i])\n",
    "#             count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 128 generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchong6/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5964428186416626 1.4191110134124756 16.387798309326172\n",
      "0 0.009365899488329887 0.0 14.7371244430542\n",
      "0 0.051138438284397125 0.0 4.9860334396362305\n",
      "0 0.014964363537728786 0.0 3.664454221725464\n",
      "0 0.0 0.1002039760351181 5.653564929962158\n",
      "0 0.0 0.0007618269883096218 2.6636598110198975\n",
      "0 0.0 0.5470526814460754 6.94040584564209\n",
      "0 0.0 0.2772418260574341 6.751776695251465\n",
      "0 0.005020281299948692 0.0 3.330946207046509\n",
      "0 0.0 0.0 3.717128276824951\n",
      "0 0.0 0.0 3.2507944107055664\n",
      "0 0.0 0.0 3.474308490753174\n",
      "0 0.0 0.28474700450897217 5.033273696899414\n",
      "0 0.0 0.0 3.643026113510132\n",
      "0 0.0 0.0 3.5737709999084473\n",
      "0 0.0 0.0 1.8947405815124512\n",
      "0 0.0 0.0 2.9227938652038574\n",
      "0 0.008621095679700375 0.18504652380943298 3.679661273956299\n",
      "0 0.0 0.0 2.882267951965332\n",
      "0 0.0 0.0478169322013855 4.573443412780762\n",
      "0 0.0 0.0 3.267589569091797\n",
      "0 0.0 0.0 4.360723495483398\n",
      "0 1.1747798919677734 0.0 6.892269134521484\n",
      "0 0.035564251244068146 5.245824813842773 4.853349685668945\n",
      "0 0.02089126594364643 0.0 3.160186767578125\n",
      "0 0.0 0.0 2.793476104736328\n",
      "0 0.0 0.0 3.03342342376709\n",
      "0 0.0011340668424963951 0.0 2.7059943675994873\n",
      "0 0.22295992076396942 0.05401964485645294 1.4193722009658813\n",
      "0 0.0 0.0 2.557994842529297\n",
      "1 0.0 0.002068594563752413 1.6240382194519043\n",
      "1 0.0 0.0 2.606736898422241\n",
      "1 0.007540675345808268 0.0 3.6172499656677246\n",
      "1 0.011029733344912529 0.0 2.9796128273010254\n",
      "1 0.0 5.631148815155029e-05 2.1326904296875\n",
      "1 0.0 0.0 2.1886658668518066\n",
      "1 0.0 0.0 2.383411169052124\n",
      "1 0.0 0.0 1.4767285585403442\n",
      "1 0.06498882919549942 0.33052051067352295 1.4462602138519287\n",
      "1 0.0 0.0 2.6619513034820557\n",
      "1 0.0 0.0 4.636020660400391\n",
      "1 0.0 0.0 4.001871109008789\n",
      "1 0.0 0.0 4.97802734375\n",
      "1 0.0 0.0 3.829289436340332\n",
      "1 0.02332136034965515 0.002290865872055292 2.2021141052246094\n",
      "1 0.0 0.0 3.1054131984710693\n",
      "1 0.0 0.00024819234386086464 2.188854694366455\n",
      "1 3.086616039276123 0.0 4.619776725769043\n",
      "1 0.0 0.0 1.728265404701233\n",
      "1 0.0 0.0 3.410250663757324\n",
      "1 0.0 0.0 4.807631492614746\n",
      "1 0.0 0.0 3.665999412536621\n",
      "1 0.0 0.0 4.156778812408447\n",
      "1 0.0020916047506034374 0.0 2.256958484649658\n",
      "1 0.0 0.0 3.460516929626465\n",
      "1 0.0 0.0 2.8374032974243164\n",
      "1 0.0 0.0 3.3317067623138428\n",
      "1 0.0 0.0 4.790419578552246\n",
      "1 0.0 0.0 3.2988898754119873\n",
      "1 0.0 0.021194525063037872 2.209643840789795\n",
      "1 0.0 0.0018546492792665958 3.4557981491088867\n",
      "1 0.0 0.0 4.0910186767578125\n",
      "2 0.0 0.017699413001537323 3.078376293182373\n",
      "2 0.0029509966261684895 0.03646300360560417 3.251483917236328\n",
      "2 0.0004571257159113884 0.0 3.389155387878418\n",
      "2 0.006936834193766117 0.0 3.2354259490966797\n",
      "2 0.0 0.0 5.368353843688965\n",
      "2 0.013405250385403633 0.04267381131649017 3.0322587490081787\n",
      "2 0.0014420305378735065 0.07224132120609283 6.197978496551514\n",
      "2 0.0 0.0077588679268956184 3.3152832984924316\n",
      "2 0.0 0.0 3.7508440017700195\n",
      "2 0.7822076678276062 0.0 5.423918724060059\n",
      "2 0.0 0.0 2.7962138652801514\n",
      "2 0.0 0.0 3.8563201427459717\n",
      "2 0.07270011305809021 0.0 4.805638313293457\n",
      "2 1.698364496231079 0.0 3.87484073638916\n",
      "2 0.0008192732930183411 0.0 4.098113059997559\n",
      "2 0.0027450593188405037 0.0 3.056631565093994\n",
      "2 1.7845332622528076 0.0 5.432355880737305\n",
      "2 0.0 0.02710140123963356 7.5678815841674805\n",
      "2 0.0 0.0 3.9546101093292236\n",
      "2 0.0 0.0 3.165459632873535\n",
      "2 0.0 0.0 4.5043487548828125\n",
      "2 0.5005708932876587 0.0 3.05159854888916\n",
      "2 0.0 0.14665918052196503 3.262359619140625\n",
      "2 0.0027228756807744503 0.0 4.403041362762451\n",
      "2 0.0 0.0 3.4290363788604736\n",
      "2 0.0 2.068081855773926 5.785972595214844\n",
      "2 0.05282996594905853 0.0 3.9810009002685547\n",
      "2 0.0 0.0 2.5991249084472656\n",
      "2 0.0 0.0 4.536728382110596\n",
      "2 0.10576178878545761 0.0 3.7430107593536377\n",
      "2 0.0 0.0 3.3894171714782715\n",
      "2 0.0 0.0 3.0269522666931152\n",
      "3 0.0 0.0 2.8015496730804443\n",
      "3 0.0 0.0 2.2951772212982178\n",
      "3 0.0 0.004380195401608944 3.625821590423584\n",
      "3 0.03246602043509483 0.3604239225387573 3.604861259460449\n",
      "3 0.0 0.0 5.421181678771973\n",
      "3 0.0 0.1285848319530487 5.588468551635742\n",
      "3 0.0050099254585802555 0.0002149771898984909 2.8186676502227783\n",
      "3 0.0 0.0 3.9674527645111084\n",
      "3 0.0 0.0 2.797046422958374\n",
      "3 0.0 0.0 2.69122314453125\n",
      "3 0.0 0.0 2.5537045001983643\n",
      "3 0.0010914779268205166 0.0 2.2251882553100586\n",
      "3 0.0 0.0 2.373090982437134\n",
      "3 0.01581873558461666 0.0 2.8875479698181152\n",
      "3 0.0 0.010945483110845089 2.4260969161987305\n",
      "3 0.0 0.09442253410816193 2.984374523162842\n",
      "3 0.011533791199326515 0.0 2.6978273391723633\n",
      "3 0.0002640816383063793 0.0 3.820404052734375\n",
      "3 0.0 0.0 3.277954578399658\n",
      "3 0.0 0.0 4.272488594055176\n",
      "3 0.0 0.0 3.5296146869659424\n",
      "3 0.0 0.0 3.0586748123168945\n",
      "3 0.011983538046479225 0.0 1.8903237581253052\n",
      "3 0.0 0.0 3.0387048721313477\n",
      "3 0.0 0.006666779518127441 1.94881272315979\n",
      "3 0.0 0.0 3.229081630706787\n",
      "3 0.0 0.0 2.609443187713623\n",
      "3 0.001271556131541729 0.015398583374917507 2.9689502716064453\n",
      "3 0.0 0.0 3.7744345664978027\n",
      "3 0.0015602633357048035 0.0 2.7751693725585938\n",
      "3 0.0 0.0001386282965540886 1.7734919786453247\n",
      "3 0.0 0.026780080050230026 4.374880790710449\n",
      "4 0.0 0.0 4.5598859786987305\n",
      "4 0.0 0.0 3.389042377471924\n",
      "4 0.37485581636428833 0.0 3.0749123096466064\n",
      "4 0.0 0.0 3.026805877685547\n",
      "4 0.0 0.0 1.8974659442901611\n",
      "4 0.0039971075020730495 0.0 5.3260040283203125\n",
      "4 0.0 0.0 3.2817282676696777\n",
      "4 0.0 0.0 4.07728385925293\n",
      "4 0.0 0.4212242662906647 5.223917484283447\n",
      "4 0.0 0.0 3.933309555053711\n",
      "4 0.0 0.0 2.342349052429199\n",
      "4 0.0 0.00018556881695985794 2.170461893081665\n",
      "4 0.861757755279541 0.0 5.372687339782715\n",
      "4 0.0 0.0 4.442374229431152\n",
      "4 0.13298916816711426 0.0 3.2902932167053223\n",
      "4 0.0024099554866552353 0.0 2.301225185394287\n",
      "4 0.00135810486972332 0.0 4.508440971374512\n",
      "4 0.0 0.08885696530342102 1.3984580039978027\n",
      "4 0.0 0.14817139506340027 3.7313480377197266\n",
      "4 0.0 1.5797829627990723 7.664013385772705\n",
      "4 0.0 0.0 2.694016218185425\n",
      "4 0.0 0.0 2.511160373687744\n",
      "4 0.0 0.0 3.481649398803711\n",
      "4 1.4316551685333252 0.0 5.021377086639404\n",
      "4 0.0 0.0 4.1591386795043945\n",
      "4 0.005897596478462219 0.0 3.371455669403076\n",
      "4 0.0 0.028112057596445084 4.845954418182373\n",
      "4 0.0 0.0 3.8197264671325684\n",
      "4 0.0 0.0 3.2840418815612793\n"
     ]
    }
   ],
   "source": [
    "# dataset = code_dataset(code_dir)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset = im_dataset('./data/img_align_celeba', output_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "encode_model.eval()\n",
    "fixed_z = torch.randn([batch_size, k]).cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, img in enumerate(dataloader):\n",
    "        img = img.cuda()\n",
    "        z = torch.randn([img.size(0), k]).cuda()\n",
    "        with torch.no_grad():\n",
    "            real_feat, _ = encode_model(img)  \n",
    "            \n",
    "        fake_feat = code_gen(z)\n",
    "        \n",
    "        real_out = code_disc(real_feat)\n",
    "        fake_out = code_disc(fake_feat.detach())\n",
    "\n",
    "        err_real = adv_criterion(real_out, True, True)\n",
    "        err_fake = adv_criterion(fake_out, False, True)\n",
    "        gan_loss = err_real + err_fake\n",
    "\n",
    "        if loss == 'wgan-gp':\n",
    "            err_wgan = gradient_penalty(real_feat, fake_feat.detach(), code_disc)\n",
    "            gan_loss += err_wgan\n",
    "\n",
    "                \n",
    "        code_disc_optimizer.zero_grad()\n",
    "        gan_loss.backward()\n",
    "        code_disc_optimizer.step()\n",
    "        \n",
    "        # generator\n",
    "        fake_out = code_disc(fake_feat)\n",
    "        adv_loss = adv_criterion(fake_out, True, False)\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        adv_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(epoch, err_real.item(), err_fake.item(), adv_loss.item())\n",
    "            generate_gan(epoch, img, fixed_z)\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        torch.save(code_gen.state_dict(), '%s/code_gen.pth'%out_dir)\n",
    "        torch.save(code_disc.state_dict(), '%s/code_disc.pth'%out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_feat.shape, real_feat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
